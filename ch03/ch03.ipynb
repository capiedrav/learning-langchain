{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Chapter 3 - RAG Part II: Chatting with Your Data\n",
    "\n",
    "## Introducing Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### Retrieving Relevant Documents\n",
    "\n",
    "A RAG system for an AI app typically follows three core stages:\n",
    "* **Indexing**: This stage involves preprocessing the external data source and storing embeddings that represent the data in a vector store where they can be easily retrieved.\n",
    "* **Retrieval**: This stage involves retrieving the relevant embeddings and data stored in the\n",
    "vector store based on a user’s query.\n",
    "* **Generation**: This stage involves synthesizing the original prompt with the retrieved relevant\n",
    "documents as one final prompt sent to the model for a prediction.\n",
    "\n",
    "Let’s run through an example from scratch again, starting with the indexing stage:"
   ],
   "id": "1d61f8ebd07be9f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T16:14:31.361454Z",
     "start_time": "2025-09-06T16:14:07.156886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# load the document, split it into chunks\n",
    "raw_documents = TextLoader(\"./rime.txt\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# define embedding model\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model=\"sentence-transformers/all-mpnet-base-v2\", # use this model to perform the embedding\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": False},\n",
    ")\n",
    "\n",
    "# vector store credentials\n",
    "connection_credentials = f\"postgresql+psycopg://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@localhost:8888/{os.getenv('POSTGRES_DB')}\"\n",
    "\n",
    "# embed each chunk and insert it into the vector store\n",
    "db = PGVector.from_documents(documents=documents, embedding=hf_embedding, connection=connection_credentials)\n"
   ],
   "id": "96354fe6b7aa63aa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The indexing stage is now complete. In order to execute the retrieval stage, we need to perform similarity search calculations—such as cosine similarity—between the user’s query and our stored embeddings, so relevant chunks of our indexed document are retrieved.\n",
    "\n",
    "The retrieval process consist of:\n",
    "1. Convert the user's query into embeddings.\n",
    "2. Calculate the embeddings in the vector store that are most similar to the user's query.\n",
    "3. Retrieve the relevant document embeddings and their corresponding text chunk.\n",
    "\n",
    "We can represent these steps programmatically using LangChain as follows:"
   ],
   "id": "b579d46ba33659f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T01:51:34.221674082Z",
     "start_time": "2025-08-25T02:12:09.799254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a retriever\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# fetch query's relevant documents\n",
    "docs = retriever.invoke(input=\"Who's the ancyent marinere?\")\n",
    "docs"
   ],
   "id": "9fe12f5245c12902",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2a80a61a-87a4-4795-8aa3-56150da2c80d', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'),\n",
       " Document(id='2301595d-ab31-45f3-882c-8d6953d8419d', metadata={'source': './rime.txt'}, page_content='But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"\\n\\n     He holds him with his skinny hand,\\n       Quoth he, there was a Ship--\\n     \"Now get thee hence, thou grey-beard Loon!\\n       \"Or my Staff shall make thee skip.\"\\n\\n     He holds him with his glittering eye--\\n       The wedding guest stood still\\n     And listens like a three year\\'s child;\\n       The Marinere hath his will.\\n\\n     The wedding-guest sate on a stone,\\n       He cannot chuse but hear:\\n     And thus spake on that ancyent man,\\n       The bright-eyed Marinere.\\n\\n     The Ship was cheer\\'d, the Harbour clear\\'d--\\n       Merrily did we drop\\n     Below the Kirk, below the Hill,\\n       Below the Light-house top.\\n\\n     The Sun came up upon the left,\\n       Out of the Sea came he:\\n     And he shone bright, and on the right\\n       Went down into the Sea.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that we are using a vector store method you haven’t seen before: ```as_retriever```. This function abstracts the logic of embedding the user’s query and the underlying similarity search calculations performed by the vector store to retrieve the relevant documents.\n",
    "\n",
    "There is also an argument ```k```, which determines the number of relevant documents to fetch from the vector store. In this example, the argument ```k``` is specified as 2. This tells the vector store to return the two most relevant documents based on the user’s query."
   ],
   "id": "7f8695fad776ad04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generating LLM Predictions Using Relevant Documents\n",
    "\n",
    "Once we’ve retrieved the relevant documents based on the user’s query, the final step is to add them to the original prompt as context and then invoke the model to generate a final output.\n",
    "\n",
    "Here’s a code example continuing on from our previous example:"
   ],
   "id": "fa8daf8c2a32730e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T16:27:40.525129Z",
     "start_time": "2025-09-06T16:27:39.579415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=\n",
    "    \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=0.0)\n"
   ],
   "id": "4483a484e79923f0",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: Based solely on the provided context, the \"ancyent Marinere\" is an old sailor who stops a wedding guest and begins to tell him a story about a ship. He is described as having a \"long grey beard,\" a \"glittering eye,\" and a \"skinny hand.\" He uses his compelling gaze to force the wedding guest to listen to his tale.\n",
      "\n",
      "docs: [Document(id='6f392f3d-d539-47d7-9347-3d15ff35c80d', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'), Document(id='782ff309-c1d3-494f-9569-b464c2baf213', metadata={'source': './rime.txt'}, page_content='But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"\\n\\n     He holds him with his skinny hand,\\n       Quoth he, there was a Ship--\\n     \"Now get thee hence, thou grey-beard Loon!\\n       \"Or my Staff shall make thee skip.\"\\n\\n     He holds him with his glittering eye--\\n       The wedding guest stood still\\n     And listens like a three year\\'s child;\\n       The Marinere hath his will.\\n\\n     The wedding-guest sate on a stone,\\n       He cannot chuse but hear:\\n     And thus spake on that ancyent man,\\n       The bright-eyed Marinere.\\n\\n     The Ship was cheer\\'d, the Harbour clear\\'d--\\n       Merrily did we drop\\n     Below the Kirk, below the Hill,\\n       Below the Light-house top.\\n\\n     The Sun came up upon the left,\\n       Out of the Sea came he:\\n     And he shone bright, and on the right\\n       Went down into the Sea.')]\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "# fetch relevant documents\n",
    "question = \"\"\"Who's the ancyent marinere?\"\"\"\n",
    "docs = retriever.invoke(input=question) # get_relevant_documents method is deprecated use invoke instead\n",
    "\n",
    "# run the workflow\n",
    "answer = chain.invoke(input={\"context\": docs, \"question\": question})\n",
    "print(f\"answer: {answer.content}\\n\\ndocs: {docs}\")"
   ],
   "id": "b344772fb3a3a56f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note the following changes:\n",
    "* We implement dynamic context and question variables into our prompt, which allows us to define a ```ChatPromptTemplate``` the model can use to generate a response.\n",
    "* We define a ```DeepSeek``` interface to act as our LLM. ```Temperature``` is set to ```0``` to eliminate the creativity in outputs from the model.\n",
    "* We create a chain to compose the prompt and LLM. A reminder: the ```|``` (pipe) operator takes the output of prompt and uses it as the input to llm.\n",
    "* We invoke the chain passing in the context variable (our retrieved relevant docs) and the user’s question to generate a final output.\n",
    "\n",
    "We can encapsulate this retrieval logic in a single function:"
   ],
   "id": "5a3c08ee252c6f13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T19:01:40.039514Z",
     "start_time": "2025-09-02T19:01:40.036003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def qa(question: str) -> dict[str, Any]:\n",
    "\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input=question)\n",
    "    # prepare prompt\n",
    "    formatted_prompt =  prompt.invoke(input={\"context\": docs, \"question\": question})\n",
    "\n",
    "    answer = llm.invoke(input=formatted_prompt) # return llm's answer\n",
    "\n",
    "    return {\"answer\": answer, \"docs\": docs} # return answer and relevant docs"
   ],
   "id": "a53cae24b2a86adb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: Based solely on the provided context, the ship's journey is described in the \"Argument\" section as follows:\n",
      "\n",
      "From: The cold Country towards the South Pole\n",
      "To: The tropical Latitude of the Great Pacific Ocean\n",
      "\n",
      "relevant docs: [Document(id='6f392f3d-d539-47d7-9347-3d15ff35c80d', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'), Document(id='a9bbad5c-1ed5-40d9-9ea3-f567f3f3271b', metadata={'source': './rime.txt'}, page_content=\"The Boat came closer to the Ship,\\n       But I ne spake ne stirr'd!\\n     The Boat came close beneath the Ship,\\n       And strait a sound was heard!\\n\\n     Under the water it rumbled on,\\n       Still louder and more dread:\\n     It reach'd the Ship, it split the bay;\\n       The Ship went down like lead.\\n\\n     Stunn'd by that loud and dreadful sound,\\n       Which sky and ocean smote:\\n     Like one that hath been seven days drown'd\\n       My body lay afloat:\\n     But, swift as dreams, myself I found\\n       Within the Pilot's boat.\\n\\n     Upon the whirl, where sank the Ship,\\n       The boat spun round and round:\\n     And all was still, save that the hill\\n       Was telling of the sound.\\n\\n     I mov'd my lips: the Pilot shriek'd\\n       And fell down in a fit.\\n     The Holy Hermit rais'd his eyes\\n       And pray'd where he did sit.\")]\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "response = qa.invoke(input=\"From where to where was the ship sailing?\")\n",
    "\n",
    "print(f\"answer: {response['answer'].content}\\n\\nrelevant docs: {response['docs']}\")"
   ],
   "id": "f2400baf06b799"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice how we now have a new runnable ```qa``` function that can be called with just a question and takes care to first fetch the relevant docs for context, format them into the prompt, and finally generate the answer. The ```@chain``` decorator turns the function into a ```runnable``` chain. This notion of encapsulating multiple steps into a single function will be key to building interesting apps with LLMs.",
   "id": "8ff6ae01c4f11675"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Query Transformation\n",
    "\n",
    "One of the major problems with a basic RAG system is that it relies too heavily on the quality of a user’s query to generate an accurate output. In a production setting, a user is likely to construct their query in an incomplete, ambiguous, or poorly worded manner that leads to model hallucination.\n",
    "\n",
    "_Query transformation_ is a subset of strategies designed to modify the user’s input to\n",
    "answer the first RAG problem question: _How do we handle the variability in the\n",
    "quality of a user’s input?_\n",
    "\n",
    "### Rewrite-Retrieve-Read (RRR)\n",
    "\n",
    "The Rewrite-Retrieve-Read strategy proposed by a Microsoft Research team simply prompts the LLM to rewrite the user’s query before performing retrieval. To illustrate, let’s return to the chain we built in the previous section, this time invoked with a poorly worded user query:"
   ],
   "id": "14f4554f7ce6c0d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T19:11:13.448452Z",
     "start_time": "2025-09-02T19:11:07.965534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = qa.invoke(input=\"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker and my house burned. From where to where was the ship sailing?\")\n",
    "\n",
    "response[\"answer\"].content"
   ],
   "id": "50b8b0d4cd6b1940",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I cannot answer that question based on the provided context. The documents describe a ship caught in a storm and sinking, but they do not contain any information about the ship's departure point or destination.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model failed to answer the question because it was distracted by the irrelevant information provided in the user’s query. Now let’s implement the RRR prompt:",
   "id": "1c96b4fc781d9a9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T19:57:23.523168Z",
     "start_time": "2025-09-02T19:57:23.519148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Extract the question from the following text, rephrasing it if bad constructed. End the question with '**'. Text: {question} answer:\"\"\")\n",
    "\n",
    "def parse_rewriter_output(message: BaseMessage) -> str:\n",
    "    return message.content.strip('\"').strip('**')\n",
    "\n",
    "rewriter = rewrite_prompt | llm | parse_rewriter_output\n",
    "\n",
    "@chain\n",
    "def qa_rrr(question: str) -> dict[str, Any]:\n",
    "    # rewrite the query\n",
    "    new_question = rewriter.invoke(input=question)\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input=new_question)\n",
    "    # format prompt\n",
    "    formatted_prompt = prompt.invoke(input={\"context\": docs, \"question\": new_question})\n",
    "\n",
    "    answer = llm.invoke(input=formatted_prompt) # return llm's answer\n",
    "\n",
    "    return {\"answer\": answer, \"docs\": docs, \"question\": question, \"new_question\": new_question} # return answer and relevant docs"
   ],
   "id": "da93d4a0f4a79c2f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T20:08:32.907937Z",
     "start_time": "2025-09-02T20:08:22.801367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = qa_rrr.invoke(input=\"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker and my house burned down. From where to where was the ship sailing? Sadly, I became homeless.\")\n",
    "\n",
    "print(f\"original question: {response['question']}\\n\\nnew question: {response['new_question']}\\n\\nanswer: {response['answer'].content}\")\n"
   ],
   "id": "f81991e8e508a8f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original question: Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker and my house burned down. From where to where was the ship sailing? Sadly, I became homeless.\n",
      "\n",
      "new question: From where to where was the ship sailing?\n",
      "\n",
      "answer: Based solely on the provided context, the ship's journey is described in the \"ARGUMENT\" section of the first document:\n",
      "\n",
      "The ship was sailing from \"the cold Country towards the South Pole\" to \"the tropical Latitude of the Great Pacific Ocean.\"\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Multi-query Retrieval\n",
    "\n",
    "A user’s single query can be insufficient to capture the full scope of information required to answer the query comprehensively. The multi-query retrieval strategy resolves this problem by instructing an LLM to generate multiple queries based on a user’s initial query, executing a parallel retrieval of each query from the data source and then inserting the retrieved results as prompt context to generate a final model output.\n",
    "\n",
    "This strategy is particularly useful for use cases where a single question may rely on multiple perspectives to provide a comprehensive answer. Here’s a code example of multi-query retrieval in action:"
   ],
   "id": "bfa3bd77d6e0aba4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T17:02:41.648777Z",
     "start_time": "2025-09-06T17:02:41.644967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "perspectives_prompt = ChatPromptTemplate.from_template(\"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question, to retrieve relevant documents for a vector database. By generating multiple perspectives on the user question, your goal is to help the user to overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\"\"\")\n",
    "\n",
    "def parse_queries_output(message: BaseMessage) -> list[str]:\n",
    "    return message.content.split(\"\\n\")\n",
    "\n",
    "query_gen = perspectives_prompt | llm | parse_queries_output"
   ],
   "id": "9e246cf77127d3e8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next we take the list of generated queries, retrieve the most relevant docs for each of them in parallel, and then combine to get the unique union of all the retrieved relevant documents:",
   "id": "bfc5713417a43cc9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T17:10:50.196051Z",
     "start_time": "2025-09-06T17:10:50.191639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def get_unique_union(document_list: list[list[Document]]) -> list[Document]:\n",
    "    # flatten list of lists, and dedupe them\n",
    "    deduped_docs = {\n",
    "        doc.page_content: doc for sublist in document_list for doc in sublist\n",
    "    }\n",
    "    # return a flat list of unique docs\n",
    "    return list(deduped_docs.values())\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | get_unique_union"
   ],
   "id": "578af7871c234868",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Because we’re retrieving documents from the same retriever with multiple (related) queries, it’s likely at least some of them are repeated. Before using them as context to answer the question, we need to deduplicate them _(dedupe)_, to end up with a single instance of each.\n",
    "\n",
    "Notice our use as well of ```.batch```, which runs all generated queries in parallel and returns a list of the results—in this case, a list of lists of ```Documents```, which we then flatten and dedupe as described earlier.\n",
    "\n",
    "The final step is to construct a prompt, including the user’s question and combined retrieved relevant documents, and a model interface to generate the prediction:"
   ],
   "id": "d171d9e976ff7f4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T17:10:54.442380Z",
     "start_time": "2025-09-06T17:10:54.439258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import chain\n",
    "from typing import Any\n",
    "\n",
    "@chain\n",
    "def multi_query_qa(input: str) -> dict[str, Any]:\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input=input)\n",
    "    formatted_prompt = prompt.invoke(input={\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(input=formatted_prompt)\n",
    "\n",
    "    return {\"answer\": answer, \"docs\": docs, \"question\": input}"
   ],
   "id": "1c718029a60d1c8",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-06T17:21:28.805470Z",
     "start_time": "2025-09-06T17:21:05.065773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run the model\n",
    "response = multi_query_qa.invoke(input=\"what are the main events described in the story of the ancyent marinere?\")\n",
    "\n",
    "print(f\"question: {response['question']}\\n\\nanswer: {response['answer'].content}\\n\\ndocs: {response['docs']}\")"
   ],
   "id": "d7135378de06d77a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: what are the main events described in the story of the ancyent marinere?\n",
      "\n",
      "answer: Based solely on the provided context, the main events described are:\n",
      "\n",
      "1.  **The Mariner Detains a Wedding Guest:** An ancient mariner stops a guest on his way to a wedding feast. Despite the guest's protests, the mariner holds him with his \"glittering eye\" and compels him to listen to a story.\n",
      "\n",
      "2.  **The Ship's Journey Begins:** The mariner begins his tale by describing the ship's departure from the harbor, sailing cheerfully past the church, hill, and lighthouse. The sun rises from the sea on the left and sets into the sea on the right.\n",
      "\n",
      "3.  **A Supernatural Event with the Dead Crew:** A strong wind roars and then suddenly drops. Under the lightning and moon, the dead crew members groan, rise, and begin to work the ship's ropes again as if alive, though they are silent and move like \"lifeless tools.\" The mariner is terrified, especially working next to the body of his brother's son.\n",
      "\n",
      "4.  **The Dawn and a Mystical Sound:** When daylight dawns, the dead crew drop their work, cluster around the mast, and sweet sounds pass from their mouths and bodies.\n",
      "\n",
      "docs: [Document(id='4eccbafb-76d8-4170-9342-9f50ca444b72', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'), Document(id='e586d9ae-1391-4490-9dd5-26c52ea7f96a', metadata={'source': './rime.txt'}, page_content='But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"\\n\\n     He holds him with his skinny hand,\\n       Quoth he, there was a Ship--\\n     \"Now get thee hence, thou grey-beard Loon!\\n       \"Or my Staff shall make thee skip.\"\\n\\n     He holds him with his glittering eye--\\n       The wedding guest stood still\\n     And listens like a three year\\'s child;\\n       The Marinere hath his will.\\n\\n     The wedding-guest sate on a stone,\\n       He cannot chuse but hear:\\n     And thus spake on that ancyent man,\\n       The bright-eyed Marinere.\\n\\n     The Ship was cheer\\'d, the Harbour clear\\'d--\\n       Merrily did we drop\\n     Below the Kirk, below the Hill,\\n       Below the Light-house top.\\n\\n     The Sun came up upon the left,\\n       Out of the Sea came he:\\n     And he shone bright, and on the right\\n       Went down into the Sea.'), Document(id='ab7336dd-122b-4129-9372-cdbbd01d222c', metadata={'source': './rime.txt'}, page_content=\"The strong wind reach'd the ship: it roar'd\\n       And dropp'd down, like a stone!\\n     Beneath the lightning and the moon\\n       The dead men gave a groan.\\n\\n     They groan'd, they stirr'd, they all uprose,\\n       Ne spake, ne mov'd their eyes:\\n     It had been strange, even in a dream\\n       To have seen those dead men rise.\\n\\n     The helmsman steerd, the ship mov'd on;\\n       Yet never a breeze up-blew;\\n     The Marineres all 'gan work the ropes,\\n       Where they were wont to do:\\n\\n     They rais'd their limbs like lifeless tools--\\n       We were a ghastly crew.\\n\\n     The body of my brother's son\\n       Stood by me knee to knee:\\n     The body and I pull'd at one rope,\\n       But he said nought to me--\\n     And I quak'd to think of my own voice\\n       How frightful it would be!\\n\\n     The day-light dawn'd--they dropp'd their arms,\\n       And cluster'd round the mast:\\n     Sweet sounds rose slowly thro' their mouths\\n       And from their bodies pass'd.\")]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice how this isn’t that different from our previous QA chains, as all the new logic for multi-query retrieval is contained in ```retrieval_chain```. This is key to making good use of these techniques—implementing each technique as a standalone chain (in this case, ```retrieval_chain```), which makes it easy to adopt them and even to combine them.",
   "id": "1112838e3b762d6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aff936b756fdda8a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
