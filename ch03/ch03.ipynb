{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Chapter 3 - RAG Part II: Chatting with Your Data\n",
    "\n",
    "## Introducing Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### Retrieving Relevant Documents\n",
    "\n",
    "A RAG system for an AI app typically follows three core stages:\n",
    "* **Indexing**: This stage involves preprocessing the external data source and storing embeddings that represent the data in a vector store where they can be easily retrieved.\n",
    "* **Retrieval**: This stage involves retrieving the relevant embeddings and data stored in the\n",
    "vector store based on a user’s query.\n",
    "* **Generation**: This stage involves synthesizing the original prompt with the retrieved relevant\n",
    "documents as one final prompt sent to the model for a prediction.\n",
    "\n",
    "Let’s run through an example from scratch again, starting with the indexing stage:"
   ],
   "id": "1d61f8ebd07be9f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T01:41:59.613590Z",
     "start_time": "2025-08-25T01:41:43.574410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# load the document, split it into chunks\n",
    "raw_documents = TextLoader(\"./rime.txt\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# define embedding model\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model=\"sentence-transformers/all-mpnet-base-v2\", # use this model to perform the embedding\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": False},\n",
    ")\n",
    "\n",
    "# vector store credentials\n",
    "connection_credentials = f\"postgresql+psycopg://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@localhost:8888/{os.getenv('POSTGRES_DB')}\"\n",
    "\n",
    "# embed each chunk and insert it into the vector store\n",
    "db = PGVector.from_documents(documents=documents, embedding=hf_embedding, connection=connection_credentials)\n"
   ],
   "id": "96354fe6b7aa63aa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The indexing stage is now complete. In order to execute the retrieval stage, we need to perform similarity search calculations—such as cosine similarity—between the user’s query and our stored embeddings, so relevant chunks of our indexed document are retrieved.\n",
    "\n",
    "The retrieval process consist of:\n",
    "1. Convert the user's query into embeddings.\n",
    "2. Calculate the embeddings in the vector store that are most similar to the user's query.\n",
    "3. Retrieve the relevant document embeddings and their corresponding text chunk.\n",
    "\n",
    "We can represent these steps programmatically using LangChain as follows:"
   ],
   "id": "b579d46ba33659f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T02:12:10.272640Z",
     "start_time": "2025-08-25T02:12:09.799254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a retriever\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# fetch query's relevant documents\n",
    "docs = retriever.invoke(input=\"Who's the ancyent marinere?\")\n",
    "docs"
   ],
   "id": "9fe12f5245c12902",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2a80a61a-87a4-4795-8aa3-56150da2c80d', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'),\n",
       " Document(id='2301595d-ab31-45f3-882c-8d6953d8419d', metadata={'source': './rime.txt'}, page_content='But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"\\n\\n     He holds him with his skinny hand,\\n       Quoth he, there was a Ship--\\n     \"Now get thee hence, thou grey-beard Loon!\\n       \"Or my Staff shall make thee skip.\"\\n\\n     He holds him with his glittering eye--\\n       The wedding guest stood still\\n     And listens like a three year\\'s child;\\n       The Marinere hath his will.\\n\\n     The wedding-guest sate on a stone,\\n       He cannot chuse but hear:\\n     And thus spake on that ancyent man,\\n       The bright-eyed Marinere.\\n\\n     The Ship was cheer\\'d, the Harbour clear\\'d--\\n       Merrily did we drop\\n     Below the Kirk, below the Hill,\\n       Below the Light-house top.\\n\\n     The Sun came up upon the left,\\n       Out of the Sea came he:\\n     And he shone bright, and on the right\\n       Went down into the Sea.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that we are using a vector store method you haven’t seen before: ```as_retriever```. This function abstracts the logic of embedding the user’s query and the underlying similarity search calculations performed by the vector store to retrieve the relevant documents.\n",
    "\n",
    "There is also an argument ```k```, which determines the number of relevant documents to fetch from the vector store. In this example, the argument ```k``` is specified as 2. This tells the vector store to return the two most relevant documents based on the user’s query."
   ],
   "id": "7f8695fad776ad04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a8d72cbc793043f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
