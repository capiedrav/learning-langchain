{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Chapter 3 - RAG Part II: Chatting with Your Data\n",
    "\n",
    "## Introducing Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### Retrieving Relevant Documents\n",
    "\n",
    "A RAG system for an AI app typically follows three core stages:\n",
    "* **Indexing**: This stage involves preprocessing the external data source and storing embeddings that represent the data in a vector store where they can be easily retrieved.\n",
    "* **Retrieval**: This stage involves retrieving the relevant embeddings and data stored in the\n",
    "vector store based on a user’s query.\n",
    "* **Generation**: This stage involves synthesizing the original prompt with the retrieved relevant\n",
    "documents as one final prompt sent to the model for a prediction.\n",
    "\n",
    "Let’s run through an example from scratch again, starting with the indexing stage:"
   ],
   "id": "1d61f8ebd07be9f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T18:50:23.668322Z",
     "start_time": "2025-09-02T18:49:55.679117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# load the document, split it into chunks\n",
    "raw_documents = TextLoader(\"./rime.txt\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# define embedding model\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model=\"sentence-transformers/all-mpnet-base-v2\", # use this model to perform the embedding\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": False},\n",
    ")\n",
    "\n",
    "# vector store credentials\n",
    "connection_credentials = f\"postgresql+psycopg://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@localhost:8888/{os.getenv('POSTGRES_DB')}\"\n",
    "\n",
    "# embed each chunk and insert it into the vector store\n",
    "db = PGVector.from_documents(documents=documents, embedding=hf_embedding, connection=connection_credentials)\n"
   ],
   "id": "96354fe6b7aa63aa",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The indexing stage is now complete. In order to execute the retrieval stage, we need to perform similarity search calculations—such as cosine similarity—between the user’s query and our stored embeddings, so relevant chunks of our indexed document are retrieved.\n",
    "\n",
    "The retrieval process consist of:\n",
    "1. Convert the user's query into embeddings.\n",
    "2. Calculate the embeddings in the vector store that are most similar to the user's query.\n",
    "3. Retrieve the relevant document embeddings and their corresponding text chunk.\n",
    "\n",
    "We can represent these steps programmatically using LangChain as follows:"
   ],
   "id": "b579d46ba33659f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T01:51:34.221674082Z",
     "start_time": "2025-08-25T02:12:09.799254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a retriever\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# fetch query's relevant documents\n",
    "docs = retriever.invoke(input=\"Who's the ancyent marinere?\")\n",
    "docs"
   ],
   "id": "9fe12f5245c12902",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='2a80a61a-87a4-4795-8aa3-56150da2c80d', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'),\n",
       " Document(id='2301595d-ab31-45f3-882c-8d6953d8419d', metadata={'source': './rime.txt'}, page_content='But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"\\n\\n     He holds him with his skinny hand,\\n       Quoth he, there was a Ship--\\n     \"Now get thee hence, thou grey-beard Loon!\\n       \"Or my Staff shall make thee skip.\"\\n\\n     He holds him with his glittering eye--\\n       The wedding guest stood still\\n     And listens like a three year\\'s child;\\n       The Marinere hath his will.\\n\\n     The wedding-guest sate on a stone,\\n       He cannot chuse but hear:\\n     And thus spake on that ancyent man,\\n       The bright-eyed Marinere.\\n\\n     The Ship was cheer\\'d, the Harbour clear\\'d--\\n       Merrily did we drop\\n     Below the Kirk, below the Hill,\\n       Below the Light-house top.\\n\\n     The Sun came up upon the left,\\n       Out of the Sea came he:\\n     And he shone bright, and on the right\\n       Went down into the Sea.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that we are using a vector store method you haven’t seen before: ```as_retriever```. This function abstracts the logic of embedding the user’s query and the underlying similarity search calculations performed by the vector store to retrieve the relevant documents.\n",
    "\n",
    "There is also an argument ```k```, which determines the number of relevant documents to fetch from the vector store. In this example, the argument ```k``` is specified as 2. This tells the vector store to return the two most relevant documents based on the user’s query."
   ],
   "id": "7f8695fad776ad04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generating LLM Predictions Using Relevant Documents\n",
    "\n",
    "Once we’ve retrieved the relevant documents based on the user’s query, the final step is to add them to the original prompt as context and then invoke the model to generate a final output.\n",
    "\n",
    "Here’s a code example continuing on from our previous example:"
   ],
   "id": "fa8daf8c2a32730e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T19:04:23.545124Z",
     "start_time": "2025-09-02T19:04:22.626186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=\n",
    "    \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=0.0)\n"
   ],
   "id": "4483a484e79923f0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: Based solely on the provided context, the \"ancyent Marinere\" is an old sailor who stops a wedding guest and begins to tell him a story about a ship. He is described as having a \"long grey beard,\" a \"glittering eye,\" and a \"skinny hand.\" He uses his compelling gaze to force the wedding guest to listen to his tale.\n",
      "\n",
      "docs: [Document(id='6f392f3d-d539-47d7-9347-3d15ff35c80d', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'), Document(id='782ff309-c1d3-494f-9569-b464c2baf213', metadata={'source': './rime.txt'}, page_content='But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"\\n\\n     He holds him with his skinny hand,\\n       Quoth he, there was a Ship--\\n     \"Now get thee hence, thou grey-beard Loon!\\n       \"Or my Staff shall make thee skip.\"\\n\\n     He holds him with his glittering eye--\\n       The wedding guest stood still\\n     And listens like a three year\\'s child;\\n       The Marinere hath his will.\\n\\n     The wedding-guest sate on a stone,\\n       He cannot chuse but hear:\\n     And thus spake on that ancyent man,\\n       The bright-eyed Marinere.\\n\\n     The Ship was cheer\\'d, the Harbour clear\\'d--\\n       Merrily did we drop\\n     Below the Kirk, below the Hill,\\n       Below the Light-house top.\\n\\n     The Sun came up upon the left,\\n       Out of the Sea came he:\\n     And he shone bright, and on the right\\n       Went down into the Sea.')]\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "# fetch relevant documents\n",
    "question = \"\"\"Who's the ancyent marinere?\"\"\"\n",
    "docs = retriever.invoke(input=question) # get_relevant_documents method is deprecated use invoke instead\n",
    "\n",
    "# run the workflow\n",
    "answer = chain.invoke(input={\"context\": docs, \"question\": question})\n",
    "print(f\"answer: {answer.content}\\n\\ndocs: {docs}\")"
   ],
   "id": "b344772fb3a3a56f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note the following changes:\n",
    "* We implement dynamic context and question variables into our prompt, which allows us to define a ```ChatPromptTemplate``` the model can use to generate a response.\n",
    "* We define a ```DeepSeek``` interface to act as our LLM. ```Temperature``` is set to ```0``` to eliminate the creativity in outputs from the model.\n",
    "* We create a chain to compose the prompt and LLM. A reminder: the ```|``` (pipe) operator takes the output of prompt and uses it as the input to llm.\n",
    "* We invoke the chain passing in the context variable (our retrieved relevant docs) and the user’s question to generate a final output.\n",
    "\n",
    "We can encapsulate this retrieval logic in a single function:"
   ],
   "id": "5a3c08ee252c6f13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T19:01:40.039514Z",
     "start_time": "2025-09-02T19:01:40.036003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def qa(question: str) -> dict[str, Any]:\n",
    "\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input=question)\n",
    "    # prepare prompt\n",
    "    formatted_prompt =  prompt.invoke(input={\"context\": docs, \"question\": question})\n",
    "\n",
    "    answer = llm.invoke(input=formatted_prompt) # return llm's answer\n",
    "\n",
    "    return {\"answer\": answer, \"docs\": docs} # return answer and relevant docs"
   ],
   "id": "a53cae24b2a86adb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: Based solely on the provided context, the ship's journey is described in the \"Argument\" section as follows:\n",
      "\n",
      "From: The cold Country towards the South Pole\n",
      "To: The tropical Latitude of the Great Pacific Ocean\n",
      "\n",
      "relevant docs: [Document(id='6f392f3d-d539-47d7-9347-3d15ff35c80d', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'), Document(id='a9bbad5c-1ed5-40d9-9ea3-f567f3f3271b', metadata={'source': './rime.txt'}, page_content=\"The Boat came closer to the Ship,\\n       But I ne spake ne stirr'd!\\n     The Boat came close beneath the Ship,\\n       And strait a sound was heard!\\n\\n     Under the water it rumbled on,\\n       Still louder and more dread:\\n     It reach'd the Ship, it split the bay;\\n       The Ship went down like lead.\\n\\n     Stunn'd by that loud and dreadful sound,\\n       Which sky and ocean smote:\\n     Like one that hath been seven days drown'd\\n       My body lay afloat:\\n     But, swift as dreams, myself I found\\n       Within the Pilot's boat.\\n\\n     Upon the whirl, where sank the Ship,\\n       The boat spun round and round:\\n     And all was still, save that the hill\\n       Was telling of the sound.\\n\\n     I mov'd my lips: the Pilot shriek'd\\n       And fell down in a fit.\\n     The Holy Hermit rais'd his eyes\\n       And pray'd where he did sit.\")]\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "response = qa.invoke(input=\"From where to where was the ship sailing?\")\n",
    "\n",
    "print(f\"answer: {response['answer'].content}\\n\\nrelevant docs: {response['docs']}\")"
   ],
   "id": "f2400baf06b799"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice how we now have a new runnable ```qa``` function that can be called with just a question and takes care to first fetch the relevant docs for context, format them into the prompt, and finally generate the answer. The ```@chain``` decorator turns the function into a ```runnable``` chain. This notion of encapsulating multiple steps into a single function will be key to building interesting apps with LLMs.",
   "id": "8ff6ae01c4f11675"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Query Transformation\n",
    "\n",
    "One of the major problems with a basic RAG system is that it relies too heavily on the quality of a user’s query to generate an accurate output. In a production setting, a user is likely to construct their query in an incomplete, ambiguous, or poorly worded manner that leads to model hallucination.\n",
    "\n",
    "_Query transformation_ is a subset of strategies designed to modify the user’s input to\n",
    "answer the first RAG problem question: _How do we handle the variability in the\n",
    "quality of a user’s input?_\n",
    "\n",
    "### Rewrite-Retrieve-Read (RRR)\n",
    "\n",
    "The Rewrite-Retrieve-Read strategy proposed by a Microsoft Research team simply prompts the LLM to rewrite the user’s query before performing retrieval. To illustrate, let’s return to the chain we built in the previous section, this time invoked with a poorly worded user query:"
   ],
   "id": "14f4554f7ce6c0d8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T19:11:13.448452Z",
     "start_time": "2025-09-02T19:11:07.965534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = qa.invoke(input=\"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker and my house burned. From where to where was the ship sailing?\")\n",
    "\n",
    "response[\"answer\"].content"
   ],
   "id": "50b8b0d4cd6b1940",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I cannot answer that question based on the provided context. The documents describe a ship caught in a storm and sinking, but they do not contain any information about the ship's departure point or destination.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model failed to answer the question because it was distracted by the irrelevant information provided in the user’s query. Now let’s implement the RRR prompt:",
   "id": "1c96b4fc781d9a9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T19:57:23.523168Z",
     "start_time": "2025-09-02T19:57:23.519148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Extract the question from the following text, rephrasing it if bad constructed. End the question with '**'. Text: {question} answer:\"\"\")\n",
    "\n",
    "def parse_rewriter_output(message: BaseMessage) -> str:\n",
    "    return message.content.strip('\"').strip('**')\n",
    "\n",
    "rewriter = rewrite_prompt | llm | parse_rewriter_output\n",
    "\n",
    "@chain\n",
    "def qa_rrr(question: str) -> dict[str, Any]:\n",
    "    # rewrite the query\n",
    "    new_question = rewriter.invoke(input=question)\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input=new_question)\n",
    "    # format prompt\n",
    "    formatted_prompt = prompt.invoke(input={\"context\": docs, \"question\": new_question})\n",
    "\n",
    "    answer = llm.invoke(input=formatted_prompt) # return llm's answer\n",
    "\n",
    "    return {\"answer\": answer, \"docs\": docs, \"question\": question, \"new_question\": new_question} # return answer and relevant docs"
   ],
   "id": "da93d4a0f4a79c2f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T20:08:32.907937Z",
     "start_time": "2025-09-02T20:08:22.801367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = qa_rrr.invoke(input=\"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker and my house burned down. From where to where was the ship sailing? Sadly, I became homeless.\")\n",
    "\n",
    "print(f\"original question: {response['question']}\\n\\nnew question: {response['new_question']}\\n\\nanswer: {response['answer'].content}\")\n"
   ],
   "id": "f81991e8e508a8f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original question: Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker and my house burned down. From where to where was the ship sailing? Sadly, I became homeless.\n",
      "\n",
      "new question: From where to where was the ship sailing?\n",
      "\n",
      "answer: Based solely on the provided context, the ship's journey is described in the \"ARGUMENT\" section of the first document:\n",
      "\n",
      "The ship was sailing from \"the cold Country towards the South Pole\" to \"the tropical Latitude of the Great Pacific Ocean.\"\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
