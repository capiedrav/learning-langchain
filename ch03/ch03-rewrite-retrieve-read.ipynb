{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e45e1a3ce56d5920"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-08T20:15:09.655229Z",
     "start_time": "2025-09-08T20:15:09.652860Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# Chapter 3: RAG Part II: Chatting with your Data\n",
    "## Query transformation\n",
    "\n",
    "One of the major problems with a basic RAG system is that it relies too heavily on the quality of a user’s query to generate an accurate output. In a production setting, a user is likely to construct their query in an incomplete, ambiguous, or poorly worded manner that leads to model hallucination.\n",
    "\n",
    "_Query transformation_ is a subset of strategies designed to modify the user’s input to\n",
    "answer the first RAG problem question: _How do we handle the variability in the\n",
    "quality of a user’s input?_\n",
    "\n",
    "**NOTE**: Do not forget to launch a new pgvector docker container before using this notebook. execute ```docker compose up -d``` in the terminal.\n",
    "\n",
    "1. Setup vector store"
   ],
   "id": "db3543b9303ac07b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:11:13.543398Z",
     "start_time": "2025-09-09T01:10:54.208252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# load the document, split it into chunks\n",
    "raw_documents = TextLoader(\"./rime.txt\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# define embedding model\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model=\"sentence-transformers/all-mpnet-base-v2\", # use this model to perform the embedding\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": False},\n",
    ")\n",
    "\n",
    "# vector store credentials\n",
    "connection_credentials = f\"postgresql+psycopg://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@localhost:8888/{os.getenv('POSTGRES_DB')}\"\n",
    "\n",
    "# embed each chunk and insert it into the vector store\n",
    "db = PGVector.from_documents(documents=documents, embedding=hf_embedding, connection=connection_credentials)"
   ],
   "id": "5a585bc696904cd9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Setup retriever as llm",
   "id": "7779c6e33f4d5fa3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:11:13.605621Z",
     "start_time": "2025-09-09T01:11:13.548400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=\n",
    "    \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=0.0)"
   ],
   "id": "10c5c456569a4fff",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Setup ```qa``` runnable",
   "id": "381422ec7ffb844b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:11:13.644664Z",
     "start_time": "2025-09-09T01:11:13.610163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def qa(question: str) -> dict[str, Any]:\n",
    "\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input=question)\n",
    "    # prepare prompt\n",
    "    formatted_prompt =  prompt.invoke(input={\"context\": docs, \"question\": question})\n",
    "\n",
    "    answer = llm.invoke(input=formatted_prompt) # return llm's answer\n",
    "\n",
    "    return {\"answer\": answer, \"docs\": docs} # return answer and relevant docs"
   ],
   "id": "aa526ac410add8b4",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. run ```qa``` runnable",
   "id": "587e39d1d025f612"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:11:20.116036Z",
     "start_time": "2025-09-09T01:11:13.662931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = qa.invoke(input=\"From where to where was the ship sailing?\")\n",
    "\n",
    "print(f\"answer: {response['answer'].content}\\n\\nrelevant docs: {response['docs']}\")"
   ],
   "id": "6a012ec2326497cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: Based solely on the provided context, the ship was sailing from \"the cold Country towards the South Pole\" to \"the tropical Latitude of the Great Pacific Ocean.\"\n",
      "\n",
      "This information is found in the Argument section of the first document.\n",
      "\n",
      "relevant docs: [Document(id='2df3523a-10be-4925-871b-38e747f8063e', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'), Document(id='d5580087-b1ac-46c4-b187-3fd0086dd83c', metadata={'source': './rime.txt'}, page_content=\"The Boat came closer to the Ship,\\n       But I ne spake ne stirr'd!\\n     The Boat came close beneath the Ship,\\n       And strait a sound was heard!\\n\\n     Under the water it rumbled on,\\n       Still louder and more dread:\\n     It reach'd the Ship, it split the bay;\\n       The Ship went down like lead.\\n\\n     Stunn'd by that loud and dreadful sound,\\n       Which sky and ocean smote:\\n     Like one that hath been seven days drown'd\\n       My body lay afloat:\\n     But, swift as dreams, myself I found\\n       Within the Pilot's boat.\\n\\n     Upon the whirl, where sank the Ship,\\n       The boat spun round and round:\\n     And all was still, save that the hill\\n       Was telling of the sound.\\n\\n     I mov'd my lips: the Pilot shriek'd\\n       And fell down in a fit.\\n     The Holy Hermit rais'd his eyes\\n       And pray'd where he did sit.\")]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Rewrite-Retrieve-Read (RRR)\n",
    "\n",
    "The Rewrite-Retrieve-Read strategy proposed by a Microsoft Research team simply prompts the LLM to rewrite the user’s query before performing retrieval. To illustrate, let’s return to the chain we built in the previous section, this time invoked with a poorly worded user query:"
   ],
   "id": "528fcb01c7770e26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:11:26.264095Z",
     "start_time": "2025-09-09T01:11:20.169404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = qa.invoke(input=\"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker and my house burned. From where to where was the ship sailing? Sadly, I became homeless.\")\n",
    "\n",
    "response[\"answer\"].content"
   ],
   "id": "9ed699219db212d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based solely on the provided context from the poem \"The Rime of the Ancient Mariner,\" there is no information about where the ship was sailing from or to.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model failed to answer the question because it was distracted by the irrelevant information provided in the user’s query. Now let’s implement the RRR prompt:",
   "id": "afa7051c8d74cbe7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:30:42.092178Z",
     "start_time": "2025-09-09T01:30:42.087529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Extract the question from the following text, rephrasing it if bad constructed. put your answer between ’**’. Question: {x}\"\"\")\n",
    "\n",
    "def parse_rewriter_output(message: BaseMessage) -> str:\n",
    "    return message.content.strip('\"').strip('**')\n",
    "\n",
    "rewriter = rewrite_prompt | llm | parse_rewriter_output\n",
    "\n",
    "@chain\n",
    "def qa_rrr(question: str) -> dict[str, Any]:\n",
    "    # rewrite the query\n",
    "    new_question = rewriter.invoke(input=question)\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input=new_question)\n",
    "    # format prompt\n",
    "    formatted_prompt = prompt.invoke(input={\"context\": docs, \"question\": new_question})\n",
    "\n",
    "    answer = llm.invoke(input=formatted_prompt) # return llm's answer\n",
    "\n",
    "    return {\"answer\": answer, \"docs\": docs, \"question\": question, \"new_question\": new_question} # return answer and relevant docs"
   ],
   "id": "83d265491daf46f6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:30:54.289738Z",
     "start_time": "2025-09-09T01:30:44.388696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = qa_rrr.invoke(input=\"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker and my house burned down. From where to where was the ship sailing? Sadly, I became homeless.\")\n",
    "\n",
    "print(f\"original question: {response['question']}\\n\\nnew question: {response['new_question']}\\n\\nanswer: {response['answer'].content}\")"
   ],
   "id": "70810729e2e3c97c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original question: Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker and my house burned down. From where to where was the ship sailing? Sadly, I became homeless.\n",
      "\n",
      "new question: From where to where was the ship sailing?\n",
      "\n",
      "answer: Based solely on the provided context, the ship's journey is described in the \"Argument\" section: it was driven from \"the cold Country towards the South Pole\" to \"the tropical Latitude of the Great Pacific Ocean.\"\n",
      "\n",
      "Therefore, the ship was sailing from the South Pole to the tropical Pacific Ocean.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**NOTE:** Do not forget to remove the pgvector container when done using this notebook. Execute ```docker compose down --volumes``` in the terminal.",
   "id": "708ea465a16b8a99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "83bec3402d0a2eb7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
