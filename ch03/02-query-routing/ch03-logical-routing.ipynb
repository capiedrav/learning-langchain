{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-23T14:52:41.387795Z",
     "start_time": "2025-09-23T14:52:41.385318Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# Chapter 3: RAG Part II: Chatting with Your Data\n",
    "\n",
    "## Query Routing\n",
    "\n",
    "Although using a single vector store is useful, the required data may live in a variety of data sources, including relational databases or other vector stores.\n",
    "\n",
    "For example, you may have two vector stores: one for LangChain Python documentation and another for LangChain JS documentation. Given a user’s question, we would like to route the query to the appropriate inferred data source to retrieve relevant docs. _Query routing_ is a strategy used to forward a user’s query to the relevant data source.\n",
    "\n",
    "### Logical Routing\n",
    "\n",
    "In logical routing, we give the LLM knowledge of the various data sources at our disposal and then let the LLM reason which data source to apply based on the user’s query.\n",
    "\n",
    "In order to achieve this, we make use of function-calling models like DeepSeek to help classify each query into one of the available routes. A _function call_ involves defining a schema that the model can use to generate arguments of a function based\n",
    "on the query. This enables us to generate structured outputs that can be used to run other functions. The following Python code defines the schema for our router based on three docs for different languages:"
   ],
   "id": "5d3e1a884508a154"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T18:52:21.107639Z",
     "start_time": "2025-09-23T18:52:21.043501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    Route a user query to the most relevant data source.\n",
    "    \"\"\"\n",
    "\n",
    "    data_source: Literal[\"python_docs\", \"js_docs\"] = Field(..., description=\"Given a user question, choose which data source would be most relevant for answering the question.\")\n",
    "\n",
    "# llm with function call\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=0)\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# prompt\n",
    "system = \"You're an expert at routing a user question to the appropriate data source. Based on the programming language the question is referring to, route it to the relevant data source.\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{question}\")])\n",
    "\n",
    "# define router\n",
    "router = prompt | structured_llm\n"
   ],
   "id": "b596e964e28301e6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now we envoke the LLM to extract the data source based on the predefined schema:",
   "id": "5d7c3430f5cf7501"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T18:52:32.115498Z",
     "start_time": "2025-09-23T18:52:27.873838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "response = router.invoke({\"question\": question})\n",
    "\n",
    "response.data_source"
   ],
   "id": "e9cd74a41cf1e158",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Once we’ve extracted the relevant data source, we can pass the value into another function to execute additional logic as required:",
   "id": "dc8a66ed5f34cb2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-23T18:59:38.033909Z",
     "start_time": "2025-09-23T18:59:38.030419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def choose_route(result: RouteQuery) -> str:\n",
    "\n",
    "    if \"python_docs\" in result.data_source.lower():\n",
    "        return \"chain for python_docs\"\n",
    "    return \"chain for js_docs\"\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ],
   "id": "e270bfc536cf1ba0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notice how we don’t do an exact string comparison but instead first turn the generated output to lowercase, and then do a substring match. This makes our chain more resilient to the LLM going off script and producing output that doesn’t quite conform to the schema we asked for.\n",
    "\n",
    "Logical routing is most suitable when you have a defined list of data sources from which relevant data can be retrieved and utilized by the LLM to generate an accurate output. These can range from vector stores to databases and even APIs."
   ],
   "id": "74aa2e752686f3f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
