{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Chapter 3 - RAG Part II: Chatting with Your Data\n",
    "\n",
    "## Introducing Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "### Retrieving Relevant Documents\n",
    "\n",
    "A RAG system for an AI app typically follows three core stages:\n",
    "* **Indexing**: This stage involves preprocessing the external data source and storing embeddings that represent the data in a vector store where they can be easily retrieved.\n",
    "* **Retrieval**: This stage involves retrieving the relevant embeddings and data stored in the\n",
    "vector store based on a user’s query.\n",
    "* **Generation**: This stage involves synthesizing the original prompt with the retrieved relevant\n",
    "documents as one final prompt sent to the model for a prediction.\n",
    "\n",
    "Let’s run through an example from scratch again, starting with the indexing stage:\n",
    "\n",
    "**NOTE**: Do not forget to launch a new pgvector docker container before using this notebook. Execute ```docker compose up -d``` in the terminal."
   ],
   "id": "1d61f8ebd07be9f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:44:28.076543Z",
     "start_time": "2025-09-09T01:44:13.007420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# load the document, split it into chunks\n",
    "raw_documents = TextLoader(\"./rime.txt\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# define embedding model\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model=\"sentence-transformers/all-mpnet-base-v2\", # use this model to perform the embedding\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": False},\n",
    ")\n",
    "\n",
    "# vector store credentials\n",
    "connection_credentials = f\"postgresql+psycopg://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@localhost:8888/{os.getenv('POSTGRES_DB')}\"\n",
    "\n",
    "# embed each chunk and insert it into the vector store\n",
    "db = PGVector.from_documents(documents=documents, embedding=hf_embedding, connection=connection_credentials)\n"
   ],
   "id": "96354fe6b7aa63aa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The indexing stage is now complete. In order to execute the retrieval stage, we need to perform similarity search calculations—such as cosine similarity—between the user’s query and our stored embeddings, so relevant chunks of our indexed document are retrieved.\n",
    "\n",
    "The retrieval process consist of:\n",
    "1. Convert the user's query into embeddings.\n",
    "2. Calculate the embeddings in the vector store that are most similar to the user's query.\n",
    "3. Retrieve the relevant document embeddings and their corresponding text chunk.\n",
    "\n",
    "We can represent these steps programmatically using LangChain as follows:"
   ],
   "id": "b579d46ba33659f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:44:28.137191Z",
     "start_time": "2025-09-09T01:44:28.082453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a retriever\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# fetch query's relevant documents\n",
    "docs = retriever.invoke(input=\"Who's the ancyent marinere?\")\n",
    "docs"
   ],
   "id": "9fe12f5245c12902",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='6b021b8c-c42e-4cdd-8a3f-b7983e58ab2d', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'),\n",
       " Document(id='5d16722a-aafc-4cd7-a3fe-a4c63487934c', metadata={'source': './rime.txt'}, page_content='But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"\\n\\n     He holds him with his skinny hand,\\n       Quoth he, there was a Ship--\\n     \"Now get thee hence, thou grey-beard Loon!\\n       \"Or my Staff shall make thee skip.\"\\n\\n     He holds him with his glittering eye--\\n       The wedding guest stood still\\n     And listens like a three year\\'s child;\\n       The Marinere hath his will.\\n\\n     The wedding-guest sate on a stone,\\n       He cannot chuse but hear:\\n     And thus spake on that ancyent man,\\n       The bright-eyed Marinere.\\n\\n     The Ship was cheer\\'d, the Harbour clear\\'d--\\n       Merrily did we drop\\n     Below the Kirk, below the Hill,\\n       Below the Light-house top.\\n\\n     The Sun came up upon the left,\\n       Out of the Sea came he:\\n     And he shone bright, and on the right\\n       Went down into the Sea.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that we are using a vector store method you haven’t seen before: ```as_retriever```. This function abstracts the logic of embedding the user’s query and the underlying similarity search calculations performed by the vector store to retrieve the relevant documents.\n",
    "\n",
    "There is also an argument ```k```, which determines the number of relevant documents to fetch from the vector store. In this example, the argument ```k``` is specified as 2. This tells the vector store to return the two most relevant documents based on the user’s query."
   ],
   "id": "7f8695fad776ad04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generating LLM Predictions Using Relevant Documents\n",
    "\n",
    "Once we’ve retrieved the relevant documents based on the user’s query, the final step is to add them to the original prompt as context and then invoke the model to generate a final output.\n",
    "\n",
    "Here’s a code example continuing on from our previous example:"
   ],
   "id": "fa8daf8c2a32730e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:44:28.908547Z",
     "start_time": "2025-09-09T01:44:28.148952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=\n",
    "    \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=0.0)\n"
   ],
   "id": "4483a484e79923f0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:44:38.183145Z",
     "start_time": "2025-09-09T01:44:28.914263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "# fetch relevant documents\n",
    "question = \"\"\"Who's the ancyent marinere?\"\"\"\n",
    "docs = retriever.invoke(input=question) # get_relevant_documents method is deprecated use invoke instead\n",
    "\n",
    "# run the workflow\n",
    "answer = chain.invoke(input={\"context\": docs, \"question\": question})\n",
    "print(f\"answer: {answer.content}\\n\\ndocs: {docs}\")"
   ],
   "id": "b344772fb3a3a56f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: Based solely on the provided context, the \"ancyent Marinere\" is an old sailor who stops a wedding guest to tell him a story. He is described as having a \"long grey beard,\" a \"glittering eye,\" and a \"skinny hand.\" He compels the wedding guest to listen to his tale against his will.\n",
      "\n",
      "docs: [Document(id='6b021b8c-c42e-4cdd-8a3f-b7983e58ab2d', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'), Document(id='5d16722a-aafc-4cd7-a3fe-a4c63487934c', metadata={'source': './rime.txt'}, page_content='But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"\\n\\n     He holds him with his skinny hand,\\n       Quoth he, there was a Ship--\\n     \"Now get thee hence, thou grey-beard Loon!\\n       \"Or my Staff shall make thee skip.\"\\n\\n     He holds him with his glittering eye--\\n       The wedding guest stood still\\n     And listens like a three year\\'s child;\\n       The Marinere hath his will.\\n\\n     The wedding-guest sate on a stone,\\n       He cannot chuse but hear:\\n     And thus spake on that ancyent man,\\n       The bright-eyed Marinere.\\n\\n     The Ship was cheer\\'d, the Harbour clear\\'d--\\n       Merrily did we drop\\n     Below the Kirk, below the Hill,\\n       Below the Light-house top.\\n\\n     The Sun came up upon the left,\\n       Out of the Sea came he:\\n     And he shone bright, and on the right\\n       Went down into the Sea.')]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note the following changes:\n",
    "* We implement dynamic context and question variables into our prompt, which allows us to define a ```ChatPromptTemplate``` the model can use to generate a response.\n",
    "* We define a ```DeepSeek``` interface to act as our LLM. ```Temperature``` is set to ```0``` to eliminate the creativity in outputs from the model.\n",
    "* We create a chain to compose the prompt and LLM. A reminder: the ```|``` (pipe) operator takes the output of prompt and uses it as the input to llm.\n",
    "* We invoke the chain passing in the context variable (our retrieved relevant docs) and the user’s question to generate a final output.\n",
    "\n",
    "We can encapsulate this retrieval logic in a single function:"
   ],
   "id": "5a3c08ee252c6f13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:44:38.244065Z",
     "start_time": "2025-09-09T01:44:38.240198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def qa(question: str) -> dict[str, Any]:\n",
    "\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(input=question)\n",
    "    # prepare prompt\n",
    "    formatted_prompt =  prompt.invoke(input={\"context\": docs, \"question\": question})\n",
    "\n",
    "    answer = llm.invoke(input=formatted_prompt) # return llm's answer\n",
    "\n",
    "    return {\"answer\": answer, \"docs\": docs} # return answer and relevant docs"
   ],
   "id": "a53cae24b2a86adb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T01:44:43.501411Z",
     "start_time": "2025-09-09T01:44:38.289899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = qa.invoke(input=\"From where to where was the ship sailing?\")\n",
    "\n",
    "print(f\"answer: {response['answer'].content}\\n\\nrelevant docs: {response['docs']}\")"
   ],
   "id": "f2400baf06b799",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: Based solely on the provided context, the ship's journey is described in the Argument section:\n",
      "\n",
      "The ship was sailing **from \"the cold Country towards the South Pole\" to \"the tropical Latitude of the Great Pacific Ocean.\"**\n",
      "\n",
      "relevant docs: [Document(id='6b021b8c-c42e-4cdd-8a3f-b7983e58ab2d', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'), Document(id='7ff7b158-72ed-4228-b1ff-02978675c155', metadata={'source': './rime.txt'}, page_content=\"The Boat came closer to the Ship,\\n       But I ne spake ne stirr'd!\\n     The Boat came close beneath the Ship,\\n       And strait a sound was heard!\\n\\n     Under the water it rumbled on,\\n       Still louder and more dread:\\n     It reach'd the Ship, it split the bay;\\n       The Ship went down like lead.\\n\\n     Stunn'd by that loud and dreadful sound,\\n       Which sky and ocean smote:\\n     Like one that hath been seven days drown'd\\n       My body lay afloat:\\n     But, swift as dreams, myself I found\\n       Within the Pilot's boat.\\n\\n     Upon the whirl, where sank the Ship,\\n       The boat spun round and round:\\n     And all was still, save that the hill\\n       Was telling of the sound.\\n\\n     I mov'd my lips: the Pilot shriek'd\\n       And fell down in a fit.\\n     The Holy Hermit rais'd his eyes\\n       And pray'd where he did sit.\")]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Notice how we now have a new runnable ```qa``` function that can be called with just a question and takes care to first fetch the relevant docs for context, format them into the prompt, and finally generate the answer. In the Python code, the ```@chain``` decorator turns the function into a runnable chain. This notion of encapsulating multiple steps into a single function will be key to building interesting apps with\n",
    "LLMs.\n",
    "\n",
    "**NOTE:** Do not forget to remove the pgvector container when done using this notebook. Execute ```docker compose down --volumes``` in the terminal."
   ],
   "id": "9cbf1b827906723"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
