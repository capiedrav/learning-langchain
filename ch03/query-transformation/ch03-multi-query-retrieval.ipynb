{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "# Chapter 3: RAG Part II: Chatting with your Data\n",
    "## Query transformation\n",
    "\n",
    "One of the major problems with a basic RAG system is that it relies too heavily on the quality of a user’s query to generate an accurate output. In a production setting, a user is likely to construct their query in an incomplete, ambiguous, or poorly worded manner that leads to model hallucination.\n",
    "\n",
    "_Query transformation_ is a subset of strategies designed to modify the user’s input to\n",
    "answer the first RAG problem question: _How do we handle the variability in the\n",
    "quality of a user’s input?_\n",
    "\n",
    "### Multi-query Retrieval\n",
    "\n",
    "A user’s single query can be insufficient to capture the full scope of information required to answer the query comprehensively. The multi-query retrieval strategy resolves this problem by instructing an LLM to generate multiple queries based on a user’s initial query, executing a parallel retrieval of each query from the data source and then inserting the retrieved results as prompt context to generate a final model output.\n",
    "\n",
    "This strategy is particularly useful for use cases where a single question may rely on multiple perspectives to provide a comprehensive answer. Here’s a code example of multi-query retrieval in action:\n",
    "\n",
    "1. Setup vector store\n",
    "\n",
    "**NOTE**: Do not forget to launch a new pgvector docker container before using this notebook. execute ```docker compose up -d``` in the terminal.\n"
   ],
   "id": "8faa4c9f83bd58a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:03:35.607408Z",
     "start_time": "2025-09-08T20:03:15.049532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# load the document, split it into chunks\n",
    "raw_documents = TextLoader(\"./rime.txt\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "# define embedding model\n",
    "hf_embedding = HuggingFaceEmbeddings(\n",
    "    model=\"sentence-transformers/all-mpnet-base-v2\", # use this model to perform the embedding\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": False},\n",
    ")\n",
    "\n",
    "# vector store credentials\n",
    "connection_credentials = f\"postgresql+psycopg://{os.getenv('POSTGRES_USER')}:{os.getenv('POSTGRES_PASSWORD')}@localhost:8888/{os.getenv('POSTGRES_DB')}\"\n",
    "\n",
    "# embed each chunk and insert it into the vector store\n",
    "db = PGVector.from_documents(documents=documents, embedding=hf_embedding, connection=connection_credentials)"
   ],
   "id": "9c2416206259bdb6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Setup retriever and llm",
   "id": "6e19ed4864a6eaa9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:03:36.552233Z",
     "start_time": "2025-09-08T20:03:35.620148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    template=\n",
    "    \"\"\"\n",
    "    Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    ")\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=0.0)"
   ],
   "id": "ff91075a1d823597",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. Setup multi-query runnable",
   "id": "a58c9714c485901a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:03:36.649686Z",
     "start_time": "2025-09-08T20:03:36.614952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "perspectives_prompt = ChatPromptTemplate.from_template(\"\"\"You are an AI language model assistant. Your task is to generate five different versions of the given user question, to retrieve relevant documents for a vector database. By generating multiple perspectives on the user question, your goal is to help the user to overcome some of the limitations of the distance-based similarity search. Provide these alternative questions separated by newlines. Original question: {question}\"\"\")\n",
    "\n",
    "def parse_queries_output(message: BaseMessage) -> list[str]:\n",
    "    return message.content.split(\"\\n\")\n",
    "\n",
    "query_gen = perspectives_prompt | llm | parse_queries_output"
   ],
   "id": "84ac5505f3592a90",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next we take the list of generated queries, retrieve the most relevant docs for each of them in parallel, and then combine to get the unique union of all the retrieved relevant documents:",
   "id": "8b1abf78653dd949"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:03:36.745192Z",
     "start_time": "2025-09-08T20:03:36.739348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "def get_unique_union(document_list: list[list[Document]]) -> list[Document]:\n",
    "    # flatten list of lists, and dedupe them\n",
    "    deduped_docs = {\n",
    "        doc.page_content: doc for sublist in document_list for doc in sublist\n",
    "    }\n",
    "    # return a flat list of unique docs\n",
    "    return list(deduped_docs.values())\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | get_unique_union"
   ],
   "id": "6f1c4f552c04c88f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Because we’re retrieving documents from the same retriever with multiple (related) queries, it’s likely at least some of them are repeated. Before using them as context to answer the question, we need to deduplicate them _(dedupe)_, to end up with a single instance of each.\n",
    "\n",
    "Notice our use as well of ```.batch```, which runs all generated queries in parallel and returns a list of the results—in this case, a list of lists of ```Documents```, which we then flatten and dedupe as described earlier.\n",
    "\n",
    "The final step is to construct a prompt, including the user’s question and combined retrieved relevant documents, and a model interface to generate the prediction:"
   ],
   "id": "8c4981a07a01f8f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:03:36.809648Z",
     "start_time": "2025-09-08T20:03:36.772103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.runnables import chain\n",
    "from typing import Any\n",
    "\n",
    "@chain\n",
    "def multi_query_qa(input: str) -> dict[str, Any]:\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input=input)\n",
    "    formatted_prompt = prompt.invoke(input={\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(input=formatted_prompt)\n",
    "\n",
    "    return {\"answer\": answer, \"docs\": docs, \"question\": input}"
   ],
   "id": "2fcddfee976af241",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. Run the model",
   "id": "3f7a219545acf3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T20:04:02.771586Z",
     "start_time": "2025-09-08T20:03:36.827897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "response = multi_query_qa.invoke(input=\"what are the main events described in the story of the ancyent marinere?\")\n",
    "\n",
    "print(f\"question: {response['question']}\\n\\nanswer: {response['answer'].content}\\n\\ndocs: {response['docs']}\")"
   ],
   "id": "1260977a46ec4f6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: what are the main events described in the story of the ancyent marinere?\n",
      "\n",
      "answer: Based solely on the provided context, the main events described are:\n",
      "\n",
      "1.  **The Mariner detains a Wedding-Guest:** The ancient mariner stops a guest on his way to a wedding feast. He holds the guest with his \"glittering eye,\" compelling him to listen to his story against his will.\n",
      "\n",
      "2.  **The Ship's Journey Begins:** The mariner's ship leaves the harbor cheerfully, sailing past the church, hill, and lighthouse. The sun rises from the sea on the left and sets on the right, indicating a southward journey.\n",
      "\n",
      "3.  **Supernatural Reanimation of the Dead:** A strong wind roars and then suddenly drops. Under the lightning and moon, the dead sailors groan, rise up, and begin to work the ropes of the ship without speaking or moving their eyes. They are described as a \"ghastly crew\" working like \"lifeless tools.\"\n",
      "\n",
      "4.  **The Mariner's Terror:** The mariner is forced to work alongside the reanimated corpse of his nephew. He is terrified at the thought of what his own voice would sound like if he were to speak in this horrifying situation.\n",
      "\n",
      "5.  **A Change at Dawn:** When daylight dawns, the reanimated crew drops their work, clusters around the mast, and sweet sounds pass from their mouths and bodies.\n",
      "\n",
      "docs: [Document(id='6d9f1bec-6971-47e4-be5c-73f65fb17121', metadata={'source': './rime.txt'}, page_content='THE RIME OF THE ANCYENT MARINERE, IN SEVEN PARTS.\\n\\nARGUMENT.\\n\\nHow a Ship having passed the Line was driven by Storms to the cold Country towards the South Pole; and how from thence she made her course to the tropical Latitude of the Great Pacific Ocean; and of the strange things that befell; and in what manner the Ancyent Marinere came back to his own Country.\\n\\nI.\\n\\n     It is an ancyent Marinere,\\n       And he stoppeth one of three:\\n     \"By thy long grey beard and thy glittering eye\\n       \"Now wherefore stoppest me?\\n\\n     \"The Bridegroom\\'s doors are open\\'d wide\\n       \"And I am next of kin;\\n     \"The Guests are met, the Feast is set,--\\n       \"May\\'st hear the merry din.--\\n\\n     But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"'), Document(id='46019637-9655-440c-8f6b-6281d77c1a77', metadata={'source': './rime.txt'}, page_content='But still he holds the wedding-guest--\\n       There was a Ship, quoth he--\\n     \"Nay, if thou\\'st got a laughsome tale,\\n       \"Marinere! come with me.\"\\n\\n     He holds him with his skinny hand,\\n       Quoth he, there was a Ship--\\n     \"Now get thee hence, thou grey-beard Loon!\\n       \"Or my Staff shall make thee skip.\"\\n\\n     He holds him with his glittering eye--\\n       The wedding guest stood still\\n     And listens like a three year\\'s child;\\n       The Marinere hath his will.\\n\\n     The wedding-guest sate on a stone,\\n       He cannot chuse but hear:\\n     And thus spake on that ancyent man,\\n       The bright-eyed Marinere.\\n\\n     The Ship was cheer\\'d, the Harbour clear\\'d--\\n       Merrily did we drop\\n     Below the Kirk, below the Hill,\\n       Below the Light-house top.\\n\\n     The Sun came up upon the left,\\n       Out of the Sea came he:\\n     And he shone bright, and on the right\\n       Went down into the Sea.'), Document(id='08382d18-7236-4b9b-8932-7ae4161afae1', metadata={'source': './rime.txt'}, page_content=\"The strong wind reach'd the ship: it roar'd\\n       And dropp'd down, like a stone!\\n     Beneath the lightning and the moon\\n       The dead men gave a groan.\\n\\n     They groan'd, they stirr'd, they all uprose,\\n       Ne spake, ne mov'd their eyes:\\n     It had been strange, even in a dream\\n       To have seen those dead men rise.\\n\\n     The helmsman steerd, the ship mov'd on;\\n       Yet never a breeze up-blew;\\n     The Marineres all 'gan work the ropes,\\n       Where they were wont to do:\\n\\n     They rais'd their limbs like lifeless tools--\\n       We were a ghastly crew.\\n\\n     The body of my brother's son\\n       Stood by me knee to knee:\\n     The body and I pull'd at one rope,\\n       But he said nought to me--\\n     And I quak'd to think of my own voice\\n       How frightful it would be!\\n\\n     The day-light dawn'd--they dropp'd their arms,\\n       And cluster'd round the mast:\\n     Sweet sounds rose slowly thro' their mouths\\n       And from their bodies pass'd.\")]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**NOTE:** Do not forget to remove the pgvector container when done using this notebook. Execute ```docker compose down --volumes``` in the terminal.",
   "id": "788330c13a9c026d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1394216a4a9b3a95"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
